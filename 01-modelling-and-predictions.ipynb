{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from meteostat import Point, Daily, Stations\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\shann\\\\Documents\\\\GitHub\\\\15095-project\\\\data\\\\international_matches_FINAL.csv')\n",
    "df_city_locations = pd.read_excel('C:\\\\Users\\\\shann\\\\Documents\\\\GitHub\\\\15095-project\\\\data\\\\worldcities_excel.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>city_ascii</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>country</th>\n",
       "      <th>iso2</th>\n",
       "      <th>iso3</th>\n",
       "      <th>admin_name</th>\n",
       "      <th>capital</th>\n",
       "      <th>population</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tokyo</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>35.6839</td>\n",
       "      <td>139.7744</td>\n",
       "      <td>Japan</td>\n",
       "      <td>JP</td>\n",
       "      <td>JPN</td>\n",
       "      <td>Tōkyō</td>\n",
       "      <td>primary</td>\n",
       "      <td>39105000.0</td>\n",
       "      <td>1392685764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jakarta</td>\n",
       "      <td>Jakarta</td>\n",
       "      <td>-6.2146</td>\n",
       "      <td>106.8451</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>ID</td>\n",
       "      <td>IDN</td>\n",
       "      <td>Jakarta</td>\n",
       "      <td>primary</td>\n",
       "      <td>35362000.0</td>\n",
       "      <td>1360771077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      city city_ascii      lat       lng    country iso2 iso3 admin_name  \\\n",
       "0    Tokyo      Tokyo  35.6839  139.7744      Japan   JP  JPN      Tōkyō   \n",
       "1  Jakarta    Jakarta  -6.2146  106.8451  Indonesia   ID  IDN    Jakarta   \n",
       "\n",
       "   capital  population          id  \n",
       "0  primary  39105000.0  1392685764  \n",
       "1  primary  35362000.0  1360771077  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_locations.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city, city_ascii, country lowercase\n",
    "df_city_locations['city'] = df_city_locations['city'].str.lower()\n",
    "df_city_locations['city_ascii'] = df_city_locations['city_ascii'].str.lower()\n",
    "df_city_locations['country'] = df_city_locations['country'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract city_ascii, lat, lng, country, population from df_city_locations\n",
    "df_city_locations = df_city_locations[['city', 'lat', 'lng', 'population']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tokyo</td>\n",
       "      <td>35.6839</td>\n",
       "      <td>139.7744</td>\n",
       "      <td>39105000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jakarta</td>\n",
       "      <td>-6.2146</td>\n",
       "      <td>106.8451</td>\n",
       "      <td>35362000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      city      lat       lng  population\n",
       "0    tokyo  35.6839  139.7744  39105000.0\n",
       "1  jakarta  -6.2146  106.8451  35362000.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_locations.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>home_team</th>\n",
       "      <th>away_team</th>\n",
       "      <th>home_team_continent</th>\n",
       "      <th>away_team_continent</th>\n",
       "      <th>home_team_fifa_rank</th>\n",
       "      <th>away_team_fifa_rank</th>\n",
       "      <th>home_team_total_fifa_points</th>\n",
       "      <th>away_team_total_fifa_points</th>\n",
       "      <th>home_team_score</th>\n",
       "      <th>...</th>\n",
       "      <th>shoot_out</th>\n",
       "      <th>home_team_result</th>\n",
       "      <th>home_team_goalkeeper_score</th>\n",
       "      <th>away_team_goalkeeper_score</th>\n",
       "      <th>home_team_mean_defense_score</th>\n",
       "      <th>home_team_mean_offense_score</th>\n",
       "      <th>home_team_mean_midfield_score</th>\n",
       "      <th>away_team_mean_defense_score</th>\n",
       "      <th>away_team_mean_offense_score</th>\n",
       "      <th>away_team_mean_midfield_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-09-03</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Europe</td>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>788</td>\n",
       "      <td>535</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Draw</td>\n",
       "      <td>94.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>86.5</td>\n",
       "      <td>89.3</td>\n",
       "      <td>89.5</td>\n",
       "      <td>80.2</td>\n",
       "      <td>79.7</td>\n",
       "      <td>81.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-09-04</td>\n",
       "      <td>Austria</td>\n",
       "      <td>England</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Europe</td>\n",
       "      <td>90</td>\n",
       "      <td>7</td>\n",
       "      <td>488</td>\n",
       "      <td>732</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Draw</td>\n",
       "      <td>83.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>76.2</td>\n",
       "      <td>73.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>90.5</td>\n",
       "      <td>88.7</td>\n",
       "      <td>91.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date home_team away_team home_team_continent away_team_continent  \\\n",
       "0  2004-09-03     Spain  Scotland              Europe              Europe   \n",
       "1  2004-09-04   Austria   England              Europe              Europe   \n",
       "\n",
       "   home_team_fifa_rank  away_team_fifa_rank  home_team_total_fifa_points  \\\n",
       "0                    3                   67                          788   \n",
       "1                   90                    7                          488   \n",
       "\n",
       "   away_team_total_fifa_points  home_team_score  ...  shoot_out  \\\n",
       "0                          535                1  ...         No   \n",
       "1                          732                2  ...         No   \n",
       "\n",
       "  home_team_result home_team_goalkeeper_score away_team_goalkeeper_score  \\\n",
       "0             Draw                       94.0                       84.0   \n",
       "1             Draw                       83.0                       88.0   \n",
       "\n",
       "   home_team_mean_defense_score home_team_mean_offense_score  \\\n",
       "0                          86.5                         89.3   \n",
       "1                          76.2                         73.0   \n",
       "\n",
       "  home_team_mean_midfield_score  away_team_mean_defense_score  \\\n",
       "0                          89.5                          80.2   \n",
       "1                          74.0                          90.5   \n",
       "\n",
       "   away_team_mean_offense_score  away_team_mean_midfield_score  \n",
       "0                          79.7                           81.8  \n",
       "1                          88.7                           91.2  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variance of shoot_out column\n",
    "# make shoot_out column binary\n",
    "df['shoot_out'] = df['shoot_out'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "df['shoot_out'].var() # no variance so we can drop this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove variables that may cause leakage \n",
    "# remove variables that are not useful for modelling\n",
    "df.drop(['home_team_score', # leakage\n",
    "        'away_team_score', # leakage\n",
    "        #'date', # not useful for modelling\n",
    "        'shoot_out', # no variance\n",
    "        'neutral_location', # not useful for this problem\n",
    "        #'tournament'  # not useful for modelling\n",
    "        ],\n",
    "        inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop friendly matches in tournament column\n",
    "df = df[df['tournament'] != 'Friendly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>tournament</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vienna</td>\n",
       "      <td>Austria</td>\n",
       "      <td>FIFA World Cup qualification</td>\n",
       "      <td>2004-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zagreb</td>\n",
       "      <td>Croatia</td>\n",
       "      <td>FIFA World Cup qualification</td>\n",
       "      <td>2004-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reykjavík</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>FIFA World Cup qualification</td>\n",
       "      <td>2004-09-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        city  country                    tournament       date\n",
       "1     Vienna  Austria  FIFA World Cup qualification 2004-09-04\n",
       "2     Zagreb  Croatia  FIFA World Cup qualification 2004-09-04\n",
       "3  Reykjavík  Iceland  FIFA World Cup qualification 2004-09-04"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use country and city to find weather data for each match\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df[['city','country','tournament','date']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case city and country\n",
    "df['city'] = df['city'].str.lower()\n",
    "df['country'] = df['country'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map lat and lng to each match in df\n",
    "df = df.merge(df_city_locations, how='left', left_on=['city'], right_on=['city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vienna</td>\n",
       "      <td>48.2083</td>\n",
       "      <td>16.3725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vienna</td>\n",
       "      <td>38.8996</td>\n",
       "      <td>-77.2597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vienna</td>\n",
       "      <td>39.3240</td>\n",
       "      <td>-81.5383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     city      lat      lng\n",
       "0  vienna  48.2083  16.3725\n",
       "1  vienna  38.8996 -77.2597\n",
       "2  vienna  39.3240 -81.5383"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['city','lat','lng']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4759, 24)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'home_team', 'away_team', 'home_team_continent',\n",
       "       'away_team_continent', 'home_team_fifa_rank', 'away_team_fifa_rank',\n",
       "       'home_team_total_fifa_points', 'away_team_total_fifa_points',\n",
       "       'tournament', 'city', 'country', 'home_team_result',\n",
       "       'home_team_goalkeeper_score', 'away_team_goalkeeper_score',\n",
       "       'home_team_mean_defense_score', 'home_team_mean_offense_score',\n",
       "       'home_team_mean_midfield_score', 'away_team_mean_defense_score',\n",
       "       'away_team_mean_offense_score', 'away_team_mean_midfield_score', 'lat',\n",
       "       'lng', 'population'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Buffer dtype mismatch, expected 'Python object' but got 'long long'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:191\u001b[0m, in \u001b[0;36m_coerce_method.<locals>.wrapper\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[39mreturn\u001b[39;00m converter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m])\n\u001b[1;32m--> 191\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcannot convert the series to \u001b[39m\u001b[39m{\u001b[39;00mconverter\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot convert the series to <class 'float'>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:1085\u001b[0m, in \u001b[0;36mSeries.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   1084\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1085\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_with_engine(key, value)\n\u001b[0;32m   1086\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:1149\u001b[0m, in \u001b[0;36mSeries._set_with_engine\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[39m# this is equivalent to self._values[key] = value\u001b[39;00m\n\u001b[1;32m-> 1149\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49msetitem_inplace(loc, value)\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\base.py:190\u001b[0m, in \u001b[0;36mSingleDataManager.setitem_inplace\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    188\u001b[0m     value \u001b[39m=\u001b[39m np_can_hold_element(arr\u001b[39m.\u001b[39mdtype, value)\n\u001b[1;32m--> 190\u001b[0m arr[indexer] \u001b[39m=\u001b[39m value\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shann\\Documents\\GitHub\\15095-project\\notebooks\\xgboost-rf-lgbm-shannan.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shann/Documents/GitHub/15095-project/notebooks/xgboost-rf-lgbm-shannan.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mavg_temp\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shann/Documents/GitHub/15095-project/notebooks/xgboost-rf-lgbm-shannan.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(df)):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/shann/Documents/GitHub/15095-project/notebooks/xgboost-rf-lgbm-shannan.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     df[\u001b[39m'\u001b[39m\u001b[39mavg_temp\u001b[39m\u001b[39m'\u001b[39m][i] \u001b[39m=\u001b[39m Daily(Point(df[\u001b[39m'\u001b[39m\u001b[39mlat\u001b[39m\u001b[39m'\u001b[39m][i],df[\u001b[39m'\u001b[39m\u001b[39mlng\u001b[39m\u001b[39m'\u001b[39m][i]), start\u001b[39m=\u001b[39mdf[\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m][i], end\u001b[39m=\u001b[39mdf[\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m][i])\u001b[39m.\u001b[39mfetch()[\u001b[39m'\u001b[39m\u001b[39mtavg\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:1104\u001b[0m, in \u001b[0;36mSeries.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39msetitem_inplace(key, value)\n\u001b[0;32m   1102\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m         \u001b[39m# GH#12862 adding a new key to the Series\u001b[39;00m\n\u001b[1;32m-> 1104\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc[key] \u001b[39m=\u001b[39m value\n\u001b[0;32m   1106\u001b[0m \u001b[39mexcept\u001b[39;00m (InvalidIndexError, \u001b[39mTypeError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m   1107\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, MultiIndex):\n\u001b[0;32m   1108\u001b[0m         \u001b[39m# cases with MultiIndex don't get here bc they raise KeyError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:716\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    715\u001b[0m iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc\n\u001b[1;32m--> 716\u001b[0m iloc\u001b[39m.\u001b[39;49m_setitem_with_indexer(indexer, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1690\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1688\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_with_indexer_split_path(indexer, value, name)\n\u001b[0;32m   1689\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1690\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1929\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_block\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1923\u001b[0m     indexer \u001b[39m=\u001b[39m maybe_convert_ix(\u001b[39m*\u001b[39mindexer)  \u001b[39m# e.g. test_setitem_frame_align\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(value, ABCSeries) \u001b[39mand\u001b[39;00m name \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mdict\u001b[39m):\n\u001b[0;32m   1926\u001b[0m     \u001b[39m# TODO(EA): ExtensionBlock.setitem this causes issues with\u001b[39;00m\n\u001b[0;32m   1927\u001b[0m     \u001b[39m# setting for extensionarrays that store dicts. Need to decide\u001b[39;00m\n\u001b[0;32m   1928\u001b[0m     \u001b[39m# if it's worth supporting that.\u001b[39;00m\n\u001b[1;32m-> 1929\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_align_series(indexer, Series(value))\n\u001b[0;32m   1931\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, ABCDataFrame) \u001b[39mand\u001b[39;00m name \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1932\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_align_frame(indexer, value)\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:2138\u001b[0m, in \u001b[0;36m_iLocIndexer._align_series\u001b[1;34m(self, indexer, ser, multiindex_indexer)\u001b[0m\n\u001b[0;32m   2135\u001b[0m     \u001b[39mif\u001b[39;00m ser\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mequals(ax):\n\u001b[0;32m   2136\u001b[0m         \u001b[39mreturn\u001b[39;00m ser\u001b[39m.\u001b[39m_values\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m-> 2138\u001b[0m     \u001b[39mreturn\u001b[39;00m ser\u001b[39m.\u001b[39;49mreindex(ax)\u001b[39m.\u001b[39m_values[indexer]\n\u001b[0;32m   2140\u001b[0m \u001b[39melif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   2141\u001b[0m     ax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4672\u001b[0m, in \u001b[0;36mSeries.reindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   4668\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   4669\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m\u001b[39m passed as both positional and keyword argument\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4670\u001b[0m         )\n\u001b[0;32m   4671\u001b[0m     kwargs\u001b[39m.\u001b[39mupdate({\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index})\n\u001b[1;32m-> 4672\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mreindex(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4966\u001b[0m, in \u001b[0;36mNDFrame.reindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   4963\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_multi(axes, copy, fill_value)\n\u001b[0;32m   4965\u001b[0m \u001b[39m# perform the reindex on the axes\u001b[39;00m\n\u001b[1;32m-> 4966\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reindex_axes(\n\u001b[0;32m   4967\u001b[0m     axes, level, limit, tolerance, method, fill_value, copy\n\u001b[0;32m   4968\u001b[0m )\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreindex\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4981\u001b[0m, in \u001b[0;36mNDFrame._reindex_axes\u001b[1;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[0;32m   4978\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   4980\u001b[0m ax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis(a)\n\u001b[1;32m-> 4981\u001b[0m new_index, indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39;49mreindex(\n\u001b[0;32m   4982\u001b[0m     labels, level\u001b[39m=\u001b[39;49mlevel, limit\u001b[39m=\u001b[39;49mlimit, tolerance\u001b[39m=\u001b[39;49mtolerance, method\u001b[39m=\u001b[39;49mmethod\n\u001b[0;32m   4983\u001b[0m )\n\u001b[0;32m   4985\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis_number(a)\n\u001b[0;32m   4986\u001b[0m obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   4987\u001b[0m     {axis: [new_index, indexer]},\n\u001b[0;32m   4988\u001b[0m     fill_value\u001b[39m=\u001b[39mfill_value,\n\u001b[0;32m   4989\u001b[0m     copy\u001b[39m=\u001b[39mcopy,\n\u001b[0;32m   4990\u001b[0m     allow_dups\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   4991\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:4223\u001b[0m, in \u001b[0;36mIndex.reindex\u001b[1;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[0;32m   4214\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_unique:\n\u001b[0;32m   4215\u001b[0m             \u001b[39m# GH#42568\u001b[39;00m\n\u001b[0;32m   4216\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   4217\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mreindexing with a non-unique Index is deprecated and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4218\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mwill raise in a future version.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   4219\u001b[0m                 \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m   4220\u001b[0m                 stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   4221\u001b[0m             )\n\u001b[1;32m-> 4223\u001b[0m target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrap_reindex_result(target, indexer, preserve_names)\n\u001b[0;32m   4224\u001b[0m \u001b[39mreturn\u001b[39;00m target, indexer\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\multi.py:2520\u001b[0m, in \u001b[0;36mMultiIndex._wrap_reindex_result\u001b[1;34m(self, target, indexer, preserve_names)\u001b[0m\n\u001b[0;32m   2518\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2519\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2520\u001b[0m         target \u001b[39m=\u001b[39m MultiIndex\u001b[39m.\u001b[39;49mfrom_tuples(target)\n\u001b[0;32m   2521\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   2522\u001b[0m         \u001b[39m# not all tuples, see test_constructor_dict_multiindex_reindex_flat\u001b[39;00m\n\u001b[0;32m   2523\u001b[0m         \u001b[39mreturn\u001b[39;00m target\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\multi.py:204\u001b[0m, in \u001b[0;36mnames_compat.<locals>.new_meth\u001b[1;34m(self_or_cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m    202\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 204\u001b[0m \u001b[39mreturn\u001b[39;00m meth(self_or_cls, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\multi.py:559\u001b[0m, in \u001b[0;36mMultiIndex.from_tuples\u001b[1;34m(cls, tuples, sortorder, names)\u001b[0m\n\u001b[0;32m    556\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tuples, Index):\n\u001b[0;32m    557\u001b[0m         tuples \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(tuples\u001b[39m.\u001b[39m_values)\n\u001b[1;32m--> 559\u001b[0m     arrays \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(lib\u001b[39m.\u001b[39;49mtuples_to_object_array(tuples)\u001b[39m.\u001b[39mT)\n\u001b[0;32m    560\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tuples, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    561\u001b[0m     arrays \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(lib\u001b[39m.\u001b[39mto_object_array_tuples(tuples)\u001b[39m.\u001b[39mT)\n",
      "File \u001b[1;32mc:\\Users\\shann\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2930\u001b[0m, in \u001b[0;36mpandas._libs.lib.tuples_to_object_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Buffer dtype mismatch, expected 'Python object' but got 'long long'"
     ]
    }
   ],
   "source": [
    "# get weather data for matches\n",
    "df['avg_temp'] = 0\n",
    "for i in range(len(df)):\n",
    "    df['avg_temp'][i] = Daily(Point(df['lat'][i],df['lng'][i]), start=df['date'][i], end=df['date'][i]).fetch()['tavg']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows with missing values\n",
    "df.dropna(axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove variables that are not eeded for modelling\n",
    "df.drop(['date','city','country','tournament','lat','lng'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (XGB, Random Forest, Logistic Regression, LGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['home_team_result']\n",
    "X = df.drop(['home_team_result'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Win     0.498686\n",
       "Lose    0.269190\n",
       "Draw    0.232124\n",
       "Name: home_team_result, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline model\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding for Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Draw', 'Lose', 'Win'], dtype=object), array([0, 1, 2]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label encode the target variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "le.classes_, le.transform(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Dummies (One Hot Encoding for X's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables for categorical variables\n",
    "train_cat_cols = [col for col in X_train.columns if X_train[col].dtype == 'object']\n",
    "X_train = pd.get_dummies(X_train, columns=train_cat_cols, drop_first=True)\n",
    "test_cat_cols = [col for col in X_test.columns if X_test[col].dtype == 'object']\n",
    "X_test = pd.get_dummies(X_test, columns=test_cat_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3804, 191), (951, 184))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill any columns that are missing in X_train or X_test with 0\n",
    "for col in set(X_test.columns) - set(X_train.columns):\n",
    "    X_train[col] = 0\n",
    "\n",
    "for col in set(X_train.columns) - set(X_test.columns):\n",
    "    X_test[col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the columns are in the same order\n",
    "X_train = X_train[X_test.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save X_train, X_test, y_train, y_test\n",
    "X_train.to_csv('C:\\\\Users\\\\shann\\\\Documents\\\\GitHub\\\\15095-project\\\\data\\\\X_train.csv', index=False)\n",
    "X_test.to_csv('C:\\\\Users\\\\shann\\\\Documents\\\\GitHub\\\\15095-project\\\\data\\\\X_test.csv', index=False)\n",
    "pd.DataFrame(y_train).to_csv('C:\\\\Users\\\\shann\\\\Documents\\\\GitHub\\\\15095-project\\\\data\\\\y_train.csv', index=False)\n",
    "pd.DataFrame(y_test).to_csv('C:\\\\Users\\\\shann\\\\Documents\\\\GitHub\\\\15095-project\\\\data\\\\y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FitFailedWarning: \n",
      "35 fits failed out of a total of 70.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shann\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\shann\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\shann\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "UserWarning: One or more of the test scores are non-finite: [0.57045093        nan 0.5775517         nan 0.5736109         nan\n",
      " 0.57781658        nan 0.57729096        nan 0.57781693        nan\n",
      " 0.57781693        nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
       "                         'penalty': ['l2', 'l1']})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build logistic regression model grid search best recall score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                'penalty': ['l2','l1']}\n",
    "grid = GridSearchCV(lr, param_grid, cv=5)\n",
    "grid.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.58\n",
      "Best parameters:  {'C': 100, 'penalty': 'l2'}\n",
      "Test set score: 0.59\n",
      "Test set AUC: 0.73\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.28      0.36       221\n",
      "           1       0.51      0.48      0.49       256\n",
      "           2       0.65      0.80      0.71       474\n",
      "\n",
      "    accuracy                           0.59       951\n",
      "   macro avg       0.55      0.52      0.52       951\n",
      "weighted avg       0.57      0.59      0.57       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import roc auc score\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Test set score: {:.2f}\".format(grid.score(X_test_scaled, y_test)))\n",
    "# print roc_auc_score with ovr \n",
    "print(\"Test set AUC: {:.2f}\".format(roc_auc_score(y_test, grid.predict_proba(X_test_scaled), multi_class='ovr')))\n",
    "\n",
    "# classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, grid.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.5084708503249609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    }
   ],
   "source": [
    "# grid search logistic regression for best recall score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# define grid\n",
    "grid = dict()\n",
    "\n",
    "# define search\n",
    "search = GridSearchCV(logreg, grid, scoring='recall_macro', cv=cv, n_jobs=-1)\n",
    "\n",
    "# perform the search\n",
    "results = search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# summarize\n",
    "print('Best Score: %s' % results.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7518883031807889\n"
     ]
    }
   ],
   "source": [
    "# grid search logistic regression for best recall score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# repeated stratified k-fold cross-validation\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "# define model\n",
    "xgb_classifier = XGBClassifier()\n",
    "\n",
    "# define grid\n",
    "grid = dict()\n",
    "\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "\n",
    "# define search\n",
    "search = GridSearchCV(xgb_classifier, grid, scoring='recall_macro', cv=cv, n_jobs=-1)\n",
    "\n",
    "# perform the search\n",
    "results = search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# summarize\n",
    "print('Best Score: %s' % results.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9143680157271624"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, results.best_estimator_.predict_proba(X_test_scaled),multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.9143680157271624\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.64      0.71       221\n",
      "           1       0.75      0.74      0.75       256\n",
      "           2       0.81      0.89      0.85       474\n",
      "\n",
      "    accuracy                           0.79       951\n",
      "   macro avg       0.79      0.76      0.77       951\n",
      "weighted avg       0.79      0.79      0.79       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build xgboost model\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# roc auc score - OVR\n",
    "print(\"AUC Score:\",roc_auc_score(y_test, xgb.predict_proba(X_test_scaled), multi_class='ovr'))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, xgb.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                           callbacks=None, colsample_bylevel=1,\n",
       "                                           colsample_bynode=1,\n",
       "                                           colsample_bytree=1,\n",
       "                                           early_stopping_rounds=None,\n",
       "                                           enable_categorical=False,\n",
       "                                           eval_metric=None, gamma=0, gpu_id=-1,\n",
       "                                           grow_policy='depthwise',\n",
       "                                           importance_type=None,\n",
       "                                           interaction_constraints='',\n",
       "                                           learning_rate=0.300000012,\n",
       "                                           max_bin=256,...\n",
       "                                           predictor='auto', random_state=42,\n",
       "                                           reg_alpha=0, ...),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.5, 0.6, 0.7, 0.8,\n",
       "                                                             0.9, 1.0],\n",
       "                                        'gamma': [0, 0.25, 0.5, 1.0],\n",
       "                                        'learning_rate': [0.01, 0.05, 0.1, 0.2,\n",
       "                                                          0.3],\n",
       "                                        'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
       "                                        'min_child_weight': [1, 3, 5, 7],\n",
       "                                        'n_estimators': [100, 200, 300, 400,\n",
       "                                                         500],\n",
       "                                        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9,\n",
       "                                                      1.0]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomized search cv - xgboost\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.25, 0.5, 1.0],\n",
    "    'min_child_weight': [1, 3, 5, 7]\n",
    "}\n",
    "\n",
    "xgb_cv = XGBClassifier()\n",
    "xgb_cv = RandomizedSearchCV(estimator = xgb, param_distributions = param_grid,\n",
    "                            n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "xgb_cv.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save xgb model\n",
    "xgb_cv.best_estimator_.save_model('xgb_cv_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.9118655883151526\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.67      0.69       377\n",
      "           1       0.78      0.72      0.75       432\n",
      "           2       0.81      0.87      0.84       753\n",
      "\n",
      "    accuracy                           0.78      1562\n",
      "   macro avg       0.77      0.75      0.76      1562\n",
      "weighted avg       0.78      0.78      0.78      1562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ROC - AUC score\n",
    "print(\"AUC Score:\",roc_auc_score(y_test, xgb_cv.predict_proba(X_test_scaled), multi_class='ovr'))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, xgb_cv.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.9415639837703739\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.66      0.75       221\n",
      "           1       0.77      0.81      0.79       256\n",
      "           2       0.83      0.90      0.86       474\n",
      "\n",
      "    accuracy                           0.82       951\n",
      "   macro avg       0.82      0.79      0.80       951\n",
      "weighted avg       0.82      0.82      0.82       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build random forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# roc auc score - OVR\n",
    "print(\"AUC Score:\",roc_auc_score(y_test, rf.predict_proba(X_test_scaled), multi_class='ovr'))\n",
    "\n",
    "# print classification report\n",
    "print(classification_report(y_test, rf.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save rf model\n",
    "import pickle\n",
    "pickle.dump(rf, open('rf_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(random_state=42),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
       "                                        'max_features': ['auto', 'sqrt',\n",
       "                                                         'log2'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [100, 200, 300, 400,\n",
       "                                                         500]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomized search cv - random forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf_cv = RandomForestClassifier()\n",
    "rf_cv = RandomizedSearchCV(estimator = rf, param_distributions = param_grid,\n",
    "                            n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "rf_cv.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.8586917837197444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.20      0.33       377\n",
      "           1       0.65      0.64      0.64       432\n",
      "           2       0.66      0.92      0.77       753\n",
      "\n",
      "    accuracy                           0.67      1562\n",
      "   macro avg       0.76      0.59      0.58      1562\n",
      "weighted avg       0.73      0.67      0.63      1562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ROC - AUC score\n",
    "print(\"AUC Score:\",roc_auc_score(y_test, rf_cv.predict_proba(X_test_scaled), multi_class='ovr'))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, rf_cv.predict(X_test_scaled)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.8830836292807621\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.54      0.62       377\n",
      "           1       0.71      0.69      0.70       432\n",
      "           2       0.76      0.87      0.81       753\n",
      "\n",
      "    accuracy                           0.74      1562\n",
      "   macro avg       0.73      0.70      0.71      1562\n",
      "weighted avg       0.74      0.74      0.73      1562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# build lightgbm model\n",
    "lgbm = LGBMClassifier(random_state=42)\n",
    "lgbm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# roc auc score - OVR\n",
    "print(\"AUC Score:\",roc_auc_score(y_test, lgbm.predict_proba(X_test_scaled), multi_class='ovr'))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, lgbm.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=LGBMClassifier(random_state=42), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.5, 0.6, 0.7, 0.8,\n",
       "                                                             0.9, 1.0],\n",
       "                                        'learning_rate': [0.01, 0.05, 0.1, 0.2,\n",
       "                                                          0.3],\n",
       "                                        'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
       "                                        'min_child_weight': [1, 3, 5, 7],\n",
       "                                        'n_estimators': [100, 200, 300, 400,\n",
       "                                                         500],\n",
       "                                        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9,\n",
       "                                                      1.0]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomized search cv - lightgbm\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],  \n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'min_child_weight': [1, 3, 5, 7]\n",
    "}\n",
    "\n",
    "lgbm_cv = LGBMClassifier()\n",
    "lgbm_cv = RandomizedSearchCV(estimator = lgbm, param_distributions = param_grid,\n",
    "                            n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "lgbm_cv.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x14638796430>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save lgbm model\n",
    "lgbm_cv.best_estimator_.booster_.save_model('lgbm_cv_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.9030019639359516\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.69      0.70       377\n",
      "           1       0.77      0.73      0.75       432\n",
      "           2       0.82      0.86      0.84       753\n",
      "\n",
      "    accuracy                           0.78      1562\n",
      "   macro avg       0.77      0.76      0.76      1562\n",
      "weighted avg       0.78      0.78      0.78      1562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ROC - AUC score\n",
    "print(\"AUC Score:\",roc_auc_score(y_test, lgbm_cv.predict_proba(X_test_scaled), multi_class='ovr'))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, lgbm_cv.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World Cup Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Group Stage Data and Prepping for Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load xgb model\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_stage = pd.read_csv('C:\\\\Users\\\\shann\\\\Documents\\\\GitHub\\\\15095-project\\\\data\\\\fifa_worldcup_2022_groupstages.csv')\n",
    "group_stage_df = pd.read_csv('C:\\\\Users\\\\shann\\\\Documents\\\\GitHub\\\\15095-project\\\\data\\\\fifa_worldcup_2022_groupstages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_stage_df['city'] = group_stage_df['city'].str.lower()\n",
    "group_stage_df['date'] = pd.to_datetime(group_stage_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if city == al rayyan, population = 759,000\n",
    "# if city == doha, population = 2,382,000\n",
    "# if city == al khor, population = 214,767\n",
    "# if city == al wakrah, population = 94,272\n",
    "# if city == lusail, population = 198,600 \n",
    "# google search: population of qatar cities\n",
    "group_stage_df['population'] = group_stage_df['city'].map({'al rayyan': 759000, 'doha': 2382000, \n",
    "'al khor': 214767, 'al wakrah': 94272, 'lusail': 198600}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopy and geolocator\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"GetLoc\")\n",
    "\n",
    "# get latitude and longitude for each city\n",
    "group_stage_df['lat'] = group_stage_df['city'].apply(lambda x: geolocator.geocode(x).latitude)\n",
    "group_stage_df['lng'] = group_stage_df['city'].apply(lambda x: geolocator.geocode(x).longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# use lat and long to get weather data for matches\n",
    "group_stage_df['avg_temp'] = 0\n",
    "for i in range(len(group_stage_df)):\n",
    "    group_stage_df['avg_temp'][i] = Daily(Point(group_stage_df['lat'][i],group_stage_df['lng'][i]), \n",
    "    start=group_stage_df['date'][i], end=group_stage_df['date'][i]).fetch()['tavg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values with mean\n",
    "group_stage_df['avg_temp'] = group_stage_df['avg_temp'].fillna(group_stage_df['avg_temp'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>home_team</th>\n",
       "      <th>away_team</th>\n",
       "      <th>home_team_continent</th>\n",
       "      <th>away_team_continent</th>\n",
       "      <th>home_team_fifa_rank</th>\n",
       "      <th>away_team_fifa_rank</th>\n",
       "      <th>home_team_total_fifa_points</th>\n",
       "      <th>away_team_total_fifa_points</th>\n",
       "      <th>tournament</th>\n",
       "      <th>...</th>\n",
       "      <th>home_team_mean_offense_score</th>\n",
       "      <th>home_team_mean_midfield_score</th>\n",
       "      <th>away_team_mean_defense_score</th>\n",
       "      <th>away_team_mean_offense_score</th>\n",
       "      <th>away_team_mean_midfield_score</th>\n",
       "      <th>group</th>\n",
       "      <th>population</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>avg_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-20</td>\n",
       "      <td>Qatar</td>\n",
       "      <td>Ecuador</td>\n",
       "      <td>Asia</td>\n",
       "      <td>South America</td>\n",
       "      <td>50</td>\n",
       "      <td>44</td>\n",
       "      <td>1439.89</td>\n",
       "      <td>1464.39</td>\n",
       "      <td>FIFA World Cup</td>\n",
       "      <td>...</td>\n",
       "      <td>80.300000</td>\n",
       "      <td>77.5</td>\n",
       "      <td>73.5</td>\n",
       "      <td>78.333333</td>\n",
       "      <td>76.0</td>\n",
       "      <td>Group A</td>\n",
       "      <td>214767</td>\n",
       "      <td>25.559884</td>\n",
       "      <td>55.564020</td>\n",
       "      <td>24.895745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-21</td>\n",
       "      <td>Senegal</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Europe</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>1584.38</td>\n",
       "      <td>1694.51</td>\n",
       "      <td>FIFA World Cup</td>\n",
       "      <td>...</td>\n",
       "      <td>81.333333</td>\n",
       "      <td>79.0</td>\n",
       "      <td>86.5</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>84.5</td>\n",
       "      <td>Group A</td>\n",
       "      <td>2382000</td>\n",
       "      <td>25.285633</td>\n",
       "      <td>51.526416</td>\n",
       "      <td>24.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date home_team    away_team home_team_continent away_team_continent  \\\n",
       "0 2022-11-20     Qatar      Ecuador                Asia       South America   \n",
       "1 2022-11-21   Senegal  Netherlands              Africa              Europe   \n",
       "\n",
       "   home_team_fifa_rank  away_team_fifa_rank  home_team_total_fifa_points  \\\n",
       "0                   50                   44                      1439.89   \n",
       "1                   18                    8                      1584.38   \n",
       "\n",
       "   away_team_total_fifa_points      tournament  ...  \\\n",
       "0                      1464.39  FIFA World Cup  ...   \n",
       "1                      1694.51  FIFA World Cup  ...   \n",
       "\n",
       "  home_team_mean_offense_score home_team_mean_midfield_score  \\\n",
       "0                    80.300000                          77.5   \n",
       "1                    81.333333                          79.0   \n",
       "\n",
       "   away_team_mean_defense_score  away_team_mean_offense_score  \\\n",
       "0                          73.5                     78.333333   \n",
       "1                          86.5                     83.000000   \n",
       "\n",
       "   away_team_mean_midfield_score    group  population        lat        lng  \\\n",
       "0                           76.0  Group A      214767  25.559884  55.564020   \n",
       "1                           84.5  Group A     2382000  25.285633  51.526416   \n",
       "\n",
       "    avg_temp  \n",
       "0  24.895745  \n",
       "1  24.400000  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just to check sample rows\n",
    "group_stage_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepping Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'home_team', 'away_team', 'home_team_continent',\n",
       "       'away_team_continent', 'home_team_fifa_rank', 'away_team_fifa_rank',\n",
       "       'home_team_total_fifa_points', 'away_team_total_fifa_points',\n",
       "       'tournament', 'city', 'country', 'neutral_location',\n",
       "       'home_team_goalkeeper_score', 'away_team_goalkeeper_score',\n",
       "       'home_team_mean_defense_score', 'home_team_mean_offense_score',\n",
       "       'home_team_mean_midfield_score', 'away_team_mean_defense_score',\n",
       "       'away_team_mean_offense_score', 'away_team_mean_midfield_score',\n",
       "       'group', 'population', 'lat', 'lng', 'avg_temp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_stage_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_stage_df = group_stage_df.sort_values(by=['home_team', 'away_team'])\n",
    "group_stage_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "betting_data = pd.read_csv(\"C:\\\\Users\\\\shann\\\\Documents\\\\GitHub\\\\15095-project\\\\data\\\\group_stage_betting_odds_final_cleaned.csv\")\n",
    "\n",
    "\n",
    "# for each row i in group_stage_df, check if the home_team and away_team are the same as in the betting_data\n",
    "# if not then print row number\n",
    "for i in range(len(group_stage_df)):\n",
    "    if group_stage_df['home_team'][i] != betting_data['home_team'][i] or group_stage_df['away_team'][i] != betting_data['away_team'][i]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that are not needed for one hot encoding\n",
    "group_stage_df.drop(['date','tournament','city','country',\n",
    "'group','neutral_location','lat','lng'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables\n",
    "group_stage_df = pd.get_dummies(group_stage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "# find previously removed columns in group_stage_df and set values to 0\n",
    "for col in set(X_train.columns) - set(group_stage_df.columns):\n",
    "    group_stage_df[col] = 0\n",
    "\n",
    "group_stage_df = group_stage_df[X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_stage_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>home_team_fifa_rank</th>\n",
       "      <th>away_team_fifa_rank</th>\n",
       "      <th>home_team_total_fifa_points</th>\n",
       "      <th>away_team_total_fifa_points</th>\n",
       "      <th>home_team_goalkeeper_score</th>\n",
       "      <th>away_team_goalkeeper_score</th>\n",
       "      <th>home_team_mean_defense_score</th>\n",
       "      <th>home_team_mean_offense_score</th>\n",
       "      <th>home_team_mean_midfield_score</th>\n",
       "      <th>away_team_mean_defense_score</th>\n",
       "      <th>...</th>\n",
       "      <th>away_team_continent_Oceania</th>\n",
       "      <th>away_team_continent_South America</th>\n",
       "      <th>home_team_Congo DR</th>\n",
       "      <th>away_team_Belarus</th>\n",
       "      <th>home_team_Cabo Verde</th>\n",
       "      <th>away_team_United Arab Emirates</th>\n",
       "      <th>away_team_Benin</th>\n",
       "      <th>home_team_Togo</th>\n",
       "      <th>home_team_Angola</th>\n",
       "      <th>home_team_India</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1773.88</td>\n",
       "      <td>1644.89</td>\n",
       "      <td>84</td>\n",
       "      <td>80</td>\n",
       "      <td>82.25</td>\n",
       "      <td>89.0</td>\n",
       "      <td>84.75</td>\n",
       "      <td>78.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>1773.88</td>\n",
       "      <td>1437.78</td>\n",
       "      <td>84</td>\n",
       "      <td>70</td>\n",
       "      <td>82.25</td>\n",
       "      <td>89.0</td>\n",
       "      <td>84.75</td>\n",
       "      <td>72.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 192 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   home_team_fifa_rank  away_team_fifa_rank  home_team_total_fifa_points  \\\n",
       "0                    3                   13                      1773.88   \n",
       "1                    3                   51                      1773.88   \n",
       "\n",
       "   away_team_total_fifa_points  home_team_goalkeeper_score  \\\n",
       "0                      1644.89                          84   \n",
       "1                      1437.78                          84   \n",
       "\n",
       "   away_team_goalkeeper_score  home_team_mean_defense_score  \\\n",
       "0                          80                         82.25   \n",
       "1                          70                         82.25   \n",
       "\n",
       "   home_team_mean_offense_score  home_team_mean_midfield_score  \\\n",
       "0                          89.0                          84.75   \n",
       "1                          89.0                          84.75   \n",
       "\n",
       "   away_team_mean_defense_score  ...  away_team_continent_Oceania  \\\n",
       "0                         78.50  ...                            0   \n",
       "1                         72.75  ...                            0   \n",
       "\n",
       "   away_team_continent_South America  home_team_Congo DR  away_team_Belarus  \\\n",
       "0                                  0                   0                  0   \n",
       "1                                  0                   0                  0   \n",
       "\n",
       "   home_team_Cabo Verde  away_team_United Arab Emirates  away_team_Benin  \\\n",
       "0                     0                               0                0   \n",
       "1                     0                               0                0   \n",
       "\n",
       "   home_team_Togo  home_team_Angola  home_team_India  \n",
       "0               0                 0                0  \n",
       "1               0                 0                0  \n",
       "\n",
       "[2 rows x 192 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_stage_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "group_stage_df_scaled = scaler.transform(group_stage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load xgb model\n",
    "xgb_cv = xgb.Booster()\n",
    "xgb_cv.load_model('xgb_cv_model.json')\n",
    "xgb_cv_pred = xgb_cv.predict(xgb.DMatrix(group_stage_df_scaled))\n",
    "xgb_cv_pred = pd.DataFrame(xgb_cv_pred)\n",
    "xgb_cv_pred.rename(columns={0:'draw', 1:'home_loss', 2:'home_win'}, inplace=True)\n",
    "xgb_cv_pred['home_team'] = group_stage['home_team']\n",
    "xgb_cv_pred['away_team'] = group_stage['away_team']\n",
    "xgb_cv_pred=xgb_cv_pred[['home_team','away_team','home_win','draw','home_loss']]\n",
    "# sort new_xgb_pred by home_team and then away_team\n",
    "xgb_cv_pred = xgb_cv_pred.sort_values(by=['home_team','away_team'])\n",
    "xgb_cv_pred.to_csv('new_xgb_pred2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_xgb_pred = pd.DataFrame(new_xgb_pred)\n",
    "new_xgb_pred.columns = ['draw', 'home_loss', 'home_win']\n",
    "new_xgb_pred['home_team'] = group_stage['home_team']\n",
    "new_xgb_pred['away_team'] = group_stage['away_team']\n",
    "new_xgb_pred=new_xgb_pred[['home_team','away_team','home_win','draw','home_loss']]\n",
    "# sort new_xgb_pred by home_team and then away_team\n",
    "new_xgb_pred = new_xgb_pred.sort_values(by=['home_team','away_team'])\n",
    "new_xgb_pred.to_csv('new_xgb_pred.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4da98d95051a52da8c094cb6991c274a25cedfadc1a6d330cb8064cfb9870c7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
